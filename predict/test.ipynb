{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_csv(\"test_item.csv\")\n",
    "dfweather = pd.read_csv(\"store_weather.csv\")\n",
    "dfprom = pd.read_csv(\"sku_prom.csv\")\n",
    "dfprice = pd.read_csv(\"sku_price_and_status.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加channel\n",
    "dft = pd.concat([dftest, dftest], ignore_index=True)\n",
    "dft['channel'] = [1, 2] * (len(dftest)//2) + [2, 1] * (len(dftest)//2)\n",
    "dft = dft[['date', 'store_id', 'sku_id', 'channel']]\n",
    "dft = dft.sort_values(by=['date', 'store_id', 'sku_id'], ascending=[True, True, True], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather\n",
    "dft1 = pd.merge(dft, dfweather[['date', 'store_id', \n",
    "                                'weather_type', 'min_temperature', \n",
    "                                'max_temperature']], \n",
    "                on=['date', 'store_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price\n",
    "dft2 = pd.merge(dft1, \n",
    "                dfprice[['store_id', 'sku_id', 'date', \n",
    "                         'original_price']], \n",
    "                on=['store_id', 'sku_id', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfprom['curr_day_ratio'] = dfprom['curr_day']/dfprom['total_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft3 = pd.merge(dft2, \n",
    "                dfprom[['store_id', 'sku_id', 'date', 'channel', \n",
    "                         'threshold', 'discount_off', \n",
    "                         'curr_day_ratio', 'promotion_type']], \n",
    "                on=['store_id', 'sku_id', 'date', 'channel'], how='left')\n",
    "dft3[['threshold', 'discount_off', \n",
    "      'curr_day_ratio', 'promotion_type']] = dft3[['threshold', 'discount_off', \n",
    "                                                   'curr_day_ratio', 'promotion_type']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft3.to_csv(\"sku_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_id', 'sku_id', 'channel', 'weather_type',\n",
       "       'min_temperature', 'max_temperature', 'original_price', 'threshold',\n",
       "       'discount_off', 'curr_day_ratio', 'promotion_type', 'month', 'year',\n",
       "       'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "# 处理为读入格式\n",
    "dft = pd.read_csv(\"./sku_test.csv\")\n",
    "def get_month_year(df):\n",
    "    df['month'] = df.date.apply(lambda x: x.split('-')[1])\n",
    "    df['year'] = df.date.apply(lambda x: x.split('-')[0])\n",
    "get_month_year(dft)\n",
    "dft['date'] = pd.to_datetime(dft['date'])\n",
    "dft['day'] = dft['date'].dt.day_name()\n",
    "# dft = dft.drop(['date', 'year'], axis=1)\n",
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_c1 = np.load('../train/c1_freq.npy'); freq_c2 = np.load('../train/c2_freq.npy')\n",
    "grouped_dft = dft.groupby('channel')\n",
    "dft_c1 = grouped_dft.get_group(1).drop('channel', axis = 1)\n",
    "dft_c2 = grouped_dft.get_group(2).drop('channel', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_1 = freq_c1[0][0:111]; fc1_2 = freq_c1[0][111:293]; fc1_3 = freq_c1[0][293:1000]\n",
    "fc2_1 = freq_c2[0][0:111]; fc2_2 = freq_c2[0][111:293]; fc2_3 = freq_c2[0][293:1000]\n",
    "\n",
    "dft_c1_1 = dft_c1[dft_c1['sku_id'].isin(fc1_1)].reset_index(drop=True)\n",
    "dft_c1_2 = dft_c1[dft_c1['sku_id'].isin(fc1_2)].reset_index(drop=True)\n",
    "dft_c1_3 = dft_c1[dft_c1['sku_id'].isin(fc1_3)].reset_index(drop=True)\n",
    "dft_c2_1 = dft_c2[dft_c2['sku_id'].isin(fc2_1)].reset_index(drop=True)\n",
    "dft_c2_2 = dft_c2[dft_c2['sku_id'].isin(fc2_2)].reset_index(drop=True)\n",
    "dft_c2_3 = dft_c2[dft_c2['sku_id'].isin(fc2_3)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "dummy_variables = ['store_id', 'sku_id', 'month', 'day', 'promotion_type', 'weather_type']\n",
    "for var in dummy_variables:\n",
    "    dummy = pd.get_dummies(dft_c2_1[var], prefix = var, drop_first = False)\n",
    "    dft_c2_1 = pd.concat([dft_c2_1, dummy], axis = 1)\n",
    "dft_c2_1 = dft_c2_1.drop(dummy_variables, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../train/col_c2_1.txt', 'r') as file:\n",
    "    columns = [line.strip() for line in file]\n",
    "# 筛选出在 DataFrame 中不存在的新字段\n",
    "new_columns = [col for col in columns if col not in dft_c2_1.columns]\n",
    "# 将新字段添加到 DataFrame，并设置新列的值为 False\n",
    "for new_column in new_columns:\n",
    "    dft_c2_1[new_column] = False\n",
    "dft_c2_1 = dft_c2_1.reindex(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c2_1 = XGBRegressor()\n",
    "model_c2_1.load_model('../train/xgboost_c2_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c2_1 = model_c2_1.predict(dft_c2_1)\n",
    "pred_c2_1 = np.round(pred_c2_1, decimals = 1)\n",
    "pred_c2_1[pred_c2_1 < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_c1_1 = dft_c1[dft_c1['sku_id'].isin(fc1_1)].reset_index(drop=True)\n",
    "dft_c1_2 = dft_c1[dft_c1['sku_id'].isin(fc1_2)].reset_index(drop=True)\n",
    "dft_c1_3 = dft_c1[dft_c1['sku_id'].isin(fc1_3)].reset_index(drop=True)\n",
    "dft_c2_1 = dft_c2[dft_c2['sku_id'].isin(fc2_1)].reset_index(drop=True)\n",
    "dft_c2_2 = dft_c2[dft_c2['sku_id'].isin(fc2_2)].reset_index(drop=True)\n",
    "dft_c2_3 = dft_c2[dft_c2['sku_id'].isin(fc2_3)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sales_c2_1 = pd.DataFrame()\n",
    "pred_sales_c2_1[['store_id', 'sku_id']] = dft_c2_1[['store_id', 'sku_id']]\n",
    "pred_sales_c2_1['channel'] = 1\n",
    "pred_sales_c2_1['quantity'] = pred_c2_1\n",
    "pred_sales_c2_1.to_csv(\"pred_sales_c2_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_1 = pd.read_csv(\"pred_sales_c1_1.csv\")\n",
    "c1_2 = pd.read_csv(\"pred_sales_c1_2.csv\")\n",
    "c1_3 = pd.read_csv(\"pred_sales_c1_3.csv\")\n",
    "c2_1 = pd.read_csv(\"pred_sales_c2_1.csv\")\n",
    "c2_2 = pd.read_csv(\"pred_sales_c2_2.csv\")\n",
    "c2_3 = pd.read_csv(\"pred_sales_c2_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_1['date'] = dft_c1_1['date']\n",
    "c1_2['date'] = dft_c1_2['date']\n",
    "c1_3['date'] = dft_c1_3['date']\n",
    "c2_1['date'] = dft_c2_1['date']\n",
    "c2_2['date'] = dft_c2_2['date']\n",
    "c2_3['date'] = dft_c2_3['date']\n",
    "c1_1 = c1_1[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n",
    "c1_2 = c1_2[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n",
    "c1_3 = c1_3[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n",
    "c2_1 = c2_1[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n",
    "c2_2 = c2_2[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n",
    "c2_3 = c2_3[['date', 'store_id', 'sku_id', 'channel', 'quantity']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_1['channel'] = 2\n",
    "c2_2['channel'] = 2\n",
    "c2_3['channel'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = pd.concat([c1_1, c1_2, c1_3, c2_1, c2_2, c2_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dup = pred_result[pred_result.duplicated(subset=['date', 'store_id', 'sku_id', 'channel'], keep=False)]\n",
    "average_values = pred_dup.groupby(['date', 'store_id', 'sku_id', 'channel'])['quantity'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pred_result, average_values, on=['date', 'store_id', 'sku_id', 'channel'], \n",
    "                     how='left', suffixes=('', '_average'))\n",
    "merged_df['quantity'] = merged_df['quantity_average'].combine_first(merged_df['quantity'])\n",
    "merged_df = merged_df.drop(columns=['quantity_average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result_nr = merged_df.drop_duplicates()\n",
    "pred_result_nr = pred_result_nr.sort_values(by=['date', 'store_id', 'sku_id', 'channel'], ignore_index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result_nr.to_csv(\"pred_sales.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
