{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "c1 = pd.read_csv(\"./sku_sales_c2.csv\")\n",
    "# c1.isnull().sum().sort_values(ascending=False) # 无空行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.countplot(x='store_id', data=c1)\n",
    "plt.subplot(122)\n",
    "sns.countplot(x='sku_id', data=c1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这一步开始，对数据集进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['store_id', 'sku_id', 'original_price', 'weather_type',\n",
       "       'min_temperature', 'max_temperature', 'promotion_type', 'threshold',\n",
       "       'discount_off', 'curr_day_ratio', 'quantity', 'month', 'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Fetching the N most purchased products#\n",
    "def N_most_labels(data, variable , N , all='TRUE'):\n",
    "    vv, cc = np.unique(data[variable], return_counts=True)\n",
    "    labels_freq_pd = np.column_stack((vv, cc))\n",
    "    # labels_freq_pd = itemfreq(data[variable])\n",
    "    labels_freq_pd = labels_freq_pd[labels_freq_pd[:, 1].argsort()[::-1]] #[::-1] ==> to sort in descending order\n",
    "    if all == 'FALSE':\n",
    "        main_labels = labels_freq_pd[:,0][0:N]\n",
    "    else: \n",
    "        main_labels = labels_freq_pd[:,0][:]\n",
    "    labels_raw_np = data[variable].values #transform in numpy\n",
    "    labels_raw_np = labels_raw_np.reshape(labels_raw_np.shape[0],1)\n",
    "\n",
    "    labels_filtered_index = np.where(labels_raw_np == main_labels)\n",
    "    \n",
    "    return labels_freq_pd, labels_filtered_index\n",
    "# 修改N的值，可选出频次最高的N个商品(此处为100)\n",
    "\n",
    "label_freq, labels_filtered_index111 = N_most_labels(data = c1, variable = \"sku_id\", N = 111, all='FALSE')\n",
    "_, labels_filtered_index293 = N_most_labels(data = c1, variable = \"sku_id\", N = 293, all='FALSE')\n",
    "_, labels_filtered_index1000 = N_most_labels(data = c1, variable = \"sku_id\", N = 1000, all='FALSE')\n",
    "###\n",
    "c1_filtered_111 = c1.loc[labels_filtered_index111[0],:] # 仅保存了销量在前N位的商品\n",
    "c1_filtered_293 = c1.loc[labels_filtered_index293[0],:]\n",
    "c1_filtered_1000 = c1.loc[labels_filtered_index1000[0],:]\n",
    "\n",
    "merged_df2 = pd.merge(c1_filtered_293, c1_filtered_111, how='left', indicator=True)\n",
    "merged_df3 = pd.merge(c1_filtered_1000, c1_filtered_293, how='left', indicator=True)\n",
    "###\n",
    "c1_filtered1 = c1_filtered_111\n",
    "c1_filtered2 = merged_df2[merged_df2['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "c1_filtered3 = merged_df3[merged_df3['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "###\n",
    "def get_month_year(df):\n",
    "    df['month'] = df.date.apply(lambda x: x.split('-')[1])\n",
    "    df['year'] = df.date.apply(lambda x: x.split('-')[0])\n",
    "\n",
    "get_month_year(c1_filtered1)\n",
    "###\n",
    "c1_filtered1['date'] = pd.to_datetime(c1_filtered1['date'])\n",
    "c1_filtered1['day'] = c1_filtered1['date'].dt.day_name()\n",
    "###\n",
    "c1_filtered1 = c1_filtered1.drop(['date', 'salable_status', 'year'], axis=1)\n",
    "c1_filtered1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "sp1 = 0\n",
    "for i in range(len(label_freq)):\n",
    "    sum += label_freq[i][1]\n",
    "    if sum > 2064724/3*1:\n",
    "        sp1=i\n",
    "        break\n",
    "print(sp1)\n",
    "\n",
    "sp2 = 0\n",
    "sum = 0\n",
    "for i in range(len(label_freq)):\n",
    "    sum += label_freq[i][1]\n",
    "    if sum > 2064724*2/3:\n",
    "        sp2=i\n",
    "        break\n",
    "print(sp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variables = ['store_id', 'sku_id', 'month', 'day', 'promotion_type', 'weather_type']\n",
    "# dummy_variables = ['month','day']\n",
    "for var in dummy_variables:\n",
    "    dummy = pd.get_dummies(c1_filtered1[var], prefix = var, drop_first = False)\n",
    "    c1_filtered1 = pd.concat([c1_filtered1, dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_filtered1 = c1_filtered1.drop(dummy_variables, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_c1_3 = c1_filtered1.columns.tolist()\n",
    "col_c1_3 = [column for column in col_c1_3 if column != 'quantity']\n",
    "file_path = 'col_c2_1.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    for column in col_c1_3:\n",
    "        file.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练、测试集(4/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_filtered1 = c1_filtered1.reset_index(drop=True)  #we reset the index\n",
    "y = c1_filtered1['quantity']\n",
    "X = c1_filtered1.drop(['quantity'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "num_test = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=num_test, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "results = model.fit(X_train, y_train)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.intercept_, results.coef_)\n",
    "print(results.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# import statsmodels.formula.api as sm\n",
    "model = sm.OLS(np.array(y_train, dtype=float), np.array(X_train, dtype=float)) # 最小二乘线性回归\n",
    "results = model.fit()\n",
    "# Statsmodels gives R-like statistical output\n",
    "results.summary()\n",
    "\n",
    "# 线性回归效果很差 R=0.272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data as np.array\n",
    "features = np.array(X_train, dtype=np.float32)\n",
    "targets = np.array(y_train.values.reshape(y_train.shape[0],1), dtype=np.float32)\n",
    "features_validation= np.array(X_test, dtype=np.float32)\n",
    "targets_validation = np.array(y_test.values.reshape(y_test.shape[0],1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=256, \n",
    "                 hidden_dim3=128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.norm1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.norm2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.norm3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集\n",
    "X_train = torch.tensor(features, dtype=torch.float32).to(\"cuda:0\")\n",
    "y_train = torch.tensor(targets, dtype=torch.float32).to(\"cuda:0\")\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "batch_size = 256  # 可根据需要调整批大小\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim=19\n",
    "model = MLP(input_dim)\n",
    "model.to(\"cuda:0\")\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    loop = tqdm(dataloader, leave = True, \n",
    "                initial=0, smoothing=0.01, \n",
    "                ncols=130, dynamic_ncols=False)\n",
    "    for idx, (batch_X, batch_y) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
    "    if (epoch % 10 == 0):\n",
    "        torch.save(model.state_dict(), f\"./checkpoint/mlp2_ep_{epoch}.pth\")\n",
    "\n",
    "# Train Loss降不下去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "LSTM的训练集、测试集格式与其他模型不同，故此处进行重新整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "c1_filtered_lstm = c1.loc[labels_filtered_index[0],:] # 仅保存了销量在前N位的商品\n",
    "# c1_filtered_lstm['date'] = pd.to_datetime(c1_filtered_lstm['date'])\n",
    "c1_filtered_lstm['unix_date'] = c1_filtered_lstm['date'].apply(lambda x: datetime.timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "c1_filtered_lstm = c1_filtered_lstm.drop([\"date\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'unix_date'\n",
    "\n",
    "# 确保列名存在于DataFrame中\n",
    "if column_name in c1_filtered_lstm:\n",
    "    # 获取列的索引位置\n",
    "    column_index = c1_filtered_lstm.columns.get_loc(column_name)\n",
    "    # 重新排列列的顺序，将目标列移动到第一列\n",
    "    c1_filtered_lstm = c1_filtered_lstm[ [column_name] + [col for col in c1_filtered_lstm.columns if col != column_name] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独热编码\n",
    "dummy_variables = ['store_id', 'sku_id']\n",
    "# dummy_variables = ['month','day']\n",
    "for var in dummy_variables:\n",
    "    dummy = pd.get_dummies(c1_filtered_lstm[var], prefix = var, drop_first = False)\n",
    "    c1_filtered_lstm = pd.concat([c1_filtered_lstm, dummy], axis = 1)\n",
    "c1_filtered_lstm = c1_filtered_lstm.reset_index(drop=True)  #we reset the index\n",
    "\n",
    "# 划分训练测试\n",
    "y = c1_filtered_lstm['quantity']\n",
    "X = c1_filtered_lstm.drop(['quantity'], axis = 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "num_test = 0.20\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X, y, test_size=num_test, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "features = np.array(X_train_lstm, dtype = np.float32)\n",
    "targets = np.array(y_train_lstm.values.reshape(y_train_lstm.shape[0],1), dtype = np.float32)\n",
    "features_validation= np.array(X_test_lstm, dtype = np.float32)\n",
    "targets_validation = np.array(y_test_lstm.values.reshape(y_test_lstm.shape[0],1), dtype = np.float32)\n",
    "X_train = torch.tensor(features).to(\"cuda:0\")\n",
    "y_train = torch.tensor(targets).to(\"cuda:0\")\n",
    "\n",
    "# 构造数据集及dataloader\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "batch_size = 256  # 可根据需要调整批大小\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def nn_seq_us(dataset, B = 256):\n",
    "    print('data processing...')\n",
    "    # dataset = load_data()\n",
    "    # split\n",
    "    train = dataset[:int(len(dataset) * 0.6)]\n",
    "    val = dataset[int(len(dataset) * 0.6):int(len(dataset) * 0.8)]\n",
    "    test = dataset[int(len(dataset) * 0.8):len(dataset)]\n",
    "    m, n = np.max(train[train.columns[1]]), np.min(train[train.columns[1]])\n",
    "\n",
    "    def process(data, batch_size, shuffle):\n",
    "        load = data[data.columns[1]]\n",
    "        load = load.tolist()\n",
    "        data = data.values.tolist()\n",
    "        load = (load - n) / (m - n)\n",
    "        seq = []\n",
    "        for i in range(len(data) - 24):\n",
    "            train_seq = []\n",
    "            train_label = []\n",
    "            for j in range(i, i + 24):\n",
    "                x = [load[j]]\n",
    "                train_seq.append(x)\n",
    "            # for c in range(2, 8):\n",
    "            #     train_seq.append(data[i + 24][c])\n",
    "            train_label.append(load[i + 24])\n",
    "            train_seq = torch.FloatTensor(train_seq)\n",
    "            train_label = torch.FloatTensor(train_label).view(-1)\n",
    "            seq.append((train_seq, train_label))\n",
    "\n",
    "        # print(seq[-1])\n",
    "        seq = MyDataset(seq)\n",
    "        seq = DataLoader(dataset=seq, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=True)\n",
    "\n",
    "        return seq\n",
    "\n",
    "    Dtr = process(train, B, True)\n",
    "    Val = process(val, B, True)\n",
    "    Dte = process(test, B, False)\n",
    "\n",
    "    return Dtr, Val, Dte, m, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr, Val, Dte, m, n = nn_seq_us(c1_filtered_lstm, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.num_directions = 1 # 单向LSTM\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(\"cuda:0\")\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(\"cuda:0\")\n",
    "        output, _ = self.lstm(input_seq, (h_0, c_0))\n",
    "        pred = self.linear(output)\n",
    "        pred = pred[:, -1, :]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 124\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "model = LSTM(input_size=124, hidden_size=hidden_size, \n",
    "             num_layers=num_layers, output_size=output_size, \n",
    "             batch_size=batch_size)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(Dtr, Val, path):\n",
    "    device = \"cuda:0\"\n",
    "    input_size, hidden_size, num_layers = 1, 128, 2\n",
    "    output_size = 1\n",
    "    model = LSTM(input_size, hidden_size, num_layers, output_size, batch_size=256).to(device)\n",
    "\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,\n",
    "                                    weight_decay=1e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    # training\n",
    "    min_epochs = 10\n",
    "    best_model = None\n",
    "    min_val_loss = 5\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss = []\n",
    "        loop = tqdm(Dtr, leave = True, \n",
    "                initial=0, smoothing=0.01, \n",
    "                ncols=130, dynamic_ncols=False)\n",
    "        for idx, (seq, label) in enumerate(loop):\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(seq)\n",
    "            loss = loss_function(y_pred, label)\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f}'.format(epoch, np.mean(train_loss)))\n",
    "        model.train()\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            state = {'models': model.state_dict()}\n",
    "            torch.save(state, path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(Dtr, Val, path = './checkpoint/lstm_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(max_depth=25, tree_method=\"hist\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "train_pred = model.predict(X_train[0:600000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('R2 score using XG Boost= ',r2_score(y_test, y_pred), '/ 1.0')\n",
    "print('MSE train loss using XG Boost= ',mean_squared_error(y_train[0:600000], train_pred), '/ 0.0')\n",
    "print('MSE test loss using XG Boost= ',mean_squared_error(y_test, y_pred), '/ 0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.array(y_test[0:1000]), label = 'test')\n",
    "plt.plot(np.array(y_pred[0:1000]), label = 'pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./xgboost_c1_1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
